# import numpy as np
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
#
# from einops import rearrange
#
#
# # Gabor滤波器生成函数
# def gabor_filter(size, sigma, theta, lambd, gamma, psi=0):
#     """
#     Generate a Gabor filter with given parameters.
#     :param size: Size of the filter (height, width).
#     :param sigma: Standard deviation of the Gaussian envelope.
#     :param theta: Orientation of the filter.
#     :param lambd: Wavelength of the sinusoidal factor.
#     :param gamma: Spatial aspect ratio (x/y scaling).
#     :param psi: Phase offset.
#     :return: Gabor filter.
#     """
#     x0 = size[1] // 2
#     y0 = size[0] // 2
#     y, x = np.mgrid[-y0:y0 + 1, -x0:x0 + 1]
#
#     # Rotation
#     x_rot = x * np.cos(theta) + y * np.sin(theta)
#     y_rot = -x * np.sin(theta) + y * np.cos(theta)
#
#     # Gaussian envelope
#     envelope = np.exp(-(x_rot ** 2 + gamma ** 2 * y_rot ** 2) / (2 * sigma ** 2))
#
#     # Sinusoidal carrier
#     sinusoid = np.cos(2 * np.pi * x_rot / lambd + psi)
#
#     # Gabor filter
#     gabor = envelope * sinusoid
#     return torch.tensor(gabor, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions
#
#
# # 将Gabor滤波器应用于特征图的每个通道
# def apply_gabor_filter(x, filter_size=(5, 5), sigma=1.0, theta=0, lambd=1.0, gamma=1.0):
#     """Apply Gabor filter to each channel of input."""
#     b, c, h, w = x.size()
#     gabor_kernel = gabor_filter(filter_size, sigma, theta, lambd, gamma)
#
#     # Convert the Gabor filter to the same dtype and device as input
#     gabor_kernel = gabor_kernel.to(x.device, dtype=x.dtype)
#
#     # Apply Gabor filter on each channel
#     filtered = []
#     for i in range(c):
#         channel = x[:, i:i + 1, :, :]  # Get the current channel
#         filtered_channel = F.conv2d(channel, gabor_kernel, padding=filter_size[0] // 2)
#         filtered.append(filtered_channel)
#
#     # Stack all filtered channels
#     filtered = torch.cat(filtered, dim=1)
#
#     return filtered
#
#
# # 计算每个通道的频率响应强度
# def compute_frequency_strength(x):
#     b, c, h, w = x.size()
#     # 使用Gabor滤波器提取频率信息
#     filtered_x = apply_gabor_filter(x)
#
#     # 计算频率响应的强度：可以通过计算每个通道的L2范数来度量
#     strength = torch.norm(filtered_x, p=2, dim=[2, 3])  # L2 norm over height and width
#     return strength
#
#
# # 根据频率响应强度排序通道
# def sort_channels_by_frequency(x):
#     # 计算每个通道的频率强度
#     strength = compute_frequency_strength(x)
#
#     # 获取频率响应强度的排序索引
#     _, sorted_idx = torch.sort(strength, dim=1, descending=True)
#
#     # 按照排序索引对通道进行排序
#     x_sorted = torch.gather(x, 1, sorted_idx.unsqueeze(2).unsqueeze(3).expand(-1, -1, x.size(2), x.size(3)))
#
#     return x_sorted, sorted_idx
#
#
# # 恢复原先顺序
# def restore_original_order(x_sorted, sorted_idx):
#     # print(x_sorted,sorted_idx)
#     # 使用排序的索引恢复原先的顺序
#     _, restore_idx = torch.sort(sorted_idx, dim=1)
#     x_restored = torch.gather(x_sorted, 1,
#                               restore_idx.unsqueeze(2).unsqueeze(3).expand(-1, -1, x_sorted.size(2), x_sorted.size(3)))
#     return x_restored
#
#
# class LayerNorm(nn.Module):
#     """ LayerNorm that supports two data formats: channels_last (default) or channels_first.
#     The ordering of the dimensions in the inputs. channels_last corresponds to inputs with
#     shape (batch_size, height, width, channels) while channels_first corresponds to inputs
#     with shape (batch_size, channels, height, width).
#     """
#
#     def __init__(self, normalized_shape, eps=1e-6, data_format="channels_first"):
#         super().__init__()
#         self.weight = nn.Parameter(torch.ones(normalized_shape))
#         self.bias = nn.Parameter(torch.zeros(normalized_shape))
#         self.eps = eps
#         self.data_format = data_format
#         if self.data_format not in ["channels_last", "channels_first"]:
#             raise NotImplementedError
#         self.normalized_shape = (normalized_shape,)
#
#     def forward(self, x):
#         if self.data_format == "channels_last":
#             return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
#         elif self.data_format == "channels_first":
#             u = x.mean(1, keepdim=True)
#             s = (x - u).pow(2).mean(1, keepdim=True)
#             x = (x - u) / torch.sqrt(s + self.eps)
#             x = self.weight[:, None, None] * x + self.bias[:, None, None]
#             return x
#
#
#
# class Attention_histogram(nn.Module):
#     def __init__(self, dim, num_heads=8, bias=False, ifBox=True):
#         super(Attention_histogram, self).__init__()
#         self.factor = num_heads
#         # ifBox：布尔值，控制是否使用块级别的注意力计算。
#         # True 时，空间维度会被分成块来进行局部信息处理；
#         # False 时，保持全局结构进行长距离依赖计算。
#         self.ifBox = ifBox
#         self.num_heads = num_heads
#         self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))
#
#         # qkv 是一个 1x1 卷积层，将输入 dim 转换为 dim * 5，以便生成查询（Q）、键（K）、值（V）和其他附加信息。它的输出会包含五个通道（分别对应 Q, K, V 等）。
#         self.qkv = nn.Conv2d(dim, dim * 5, kernel_size=1, bias=bias)
#         # qkv_dwconv 是一个深度可分离卷积（depthwise convolution），即每个输入通道会单独进行卷积，输出同样是 dim * 5 通道。
#         self.qkv_dwconv = nn.Conv2d(dim * 5, dim * 5, kernel_size=3, stride=1, padding=1, groups=dim * 5, bias=bias)
#         self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)
#
#     def pad(self, x, factor):
#         hw = x.shape[-1]
#         t_pad = [0, 0] if hw % factor == 0 else [0, (hw // factor + 1) * factor - hw]
#         x = F.pad(x, t_pad, 'constant', 0)
#         return x, t_pad
#
#     def unpad(self, x, t_pad):
#         _, _, hw = x.shape
#         return x[:, :, t_pad[0]:hw - t_pad[1]]
#
#     def softmax_1(self, x, dim=-1):
#         logit = x.exp()
#         logit = logit / (logit.sum(dim, keepdim=True) + 1)
#         return logit
#
#     def normalize(self, x):
#         mu = x.mean(-2, keepdim=True)
#         sigma = x.var(-2, keepdim=True, unbiased=False)
#         return (x - mu) / torch.sqrt(sigma + 1e-5)  # * self.weight + self.bias
#
#     def reshape_attn(self, q, k, v, ifBox):
#         b, c = q.shape[:2]
#         q, t_pad = self.pad(q, self.factor)
#         k, t_pad = self.pad(k, self.factor)
#         v, t_pad = self.pad(v, self.factor)
#         hw = q.shape[-1] // self.factor
#         shape_ori = "b (head c) (factor hw)" if ifBox else "b (head c) (hw factor)"
#         # 当ifBox = True时，通过将空间维度分成块，可以更好的捕获局部信息，有助于提高模型对局部特征的关注，从而可能减少计算复杂度
#         # 当ifBox = False时，数据保持了全局的空间结构，这样可以更好的捕捉长距离依赖关系，可能会增加计算量，适合需要考虑全局上下文的情况
#         shape_tar = "b head (c factor) hw"
#
#         # 重新排列和块分布（Rearrange）：rearrange 是 einops 库中的函数，能够对张量进行灵活的维度重排，这在实现自注意力计算时非常有用。
#         # 通过 rearrange，代码将特征的空间维度重新组织，以适应多头注意力机制的要求。
#
#         # 重排 Q, K, V 的形状
#         q = rearrange(q, '{} -> {}'.format(shape_ori, shape_tar), factor=self.factor, hw=hw, head=self.num_heads)
#         k = rearrange(k, '{} -> {}'.format(shape_ori, shape_tar), factor=self.factor, hw=hw, head=self.num_heads)
#         v = rearrange(v, '{} -> {}'.format(shape_ori, shape_tar), factor=self.factor, hw=hw, head=self.num_heads)
#
#
#         q = torch.nn.functional.normalize(q, dim=-1)
#         k = torch.nn.functional.normalize(k, dim=-1)
#
#         # 计算注意力
#         attn = (q @ k.transpose(-2, -1)) * self.temperature
#         attn = self.softmax_1(attn, dim=-1)
#
#         # 恢复形状
#         out = (attn @ v)
#         out = rearrange(out, '{} -> {}'.format(shape_tar, shape_ori), factor=self.factor, hw=hw, b=b,
#                         head=self.num_heads)
#         out = self.unpad(out, t_pad)
#         return out
#
#     def forward(self, x):
#         b, c, h, w = x.shape
#
#         # Step 1: Sort the channels based on frequency information
#         x_sorted, sorted_idx = sort_channels_by_frequency(x)
#
#         # Step 2: Perform QKV decomposition
#         qkv = self.qkv_dwconv(self.qkv(x_sorted))
#         q1, k1, q2, k2, v = qkv.chunk(5, dim=1)  # b,c,x,x
#
#         # Step 3: Rearrange Q, K, V using the sorted indices
#         v, idx = v.view(b, c, -1).sort(dim=-1)
#         q1 = torch.gather(q1.view(b, c, -1), dim=2, index=idx)
#         k1 = torch.gather(k1.view(b, c, -1), dim=2, index=idx)
#         q2 = torch.gather(q2.view(b, c, -1), dim=2, index=idx)
#         k2 = torch.gather(k2.view(b, c, -1), dim=2, index=idx)
#
#         # Step 4: Compute attention (local and global)
#         out1 = self.reshape_attn(q1, k1, v, True)
#         out2 = self.reshape_attn(q2, k2, v, False)
#         # print("--------------out1: ", out1)
#         # print("--------------sorted_idx: ", sorted_idx)
#
#         out1 = torch.scatter(out1, 2, idx, out1).view(b, c, h, w)
#         out2 = torch.scatter(out2, 2, idx, out2).view(b, c, h, w)
#
#         # Step 6: Combine outputs and apply final projection
#         out = out1 * out2
#         out = self.project_out(out)
#
#         # Step 5: Recover the original channel order
#         out = restore_original_order(out, sorted_idx)
#
#         return out
#
#     # def forward(self, x):
#     #     # 1.排序：对输入 x 的前半部分进行排序，并保存排序索引 idx_h 和 idx_w，用于后续的恢复操作。
#     #     b, c, h, w = x.shape
#     #     x_sort, idx_h = x[:, :c // 2].sort(-2)
#     #     x_sort, idx_w = x_sort.sort(-1)
#     #     x[:, :c // 2] = x_sort
#     #
#     #     # 2.QKV 分解：通过卷积层 qkv 和 qkv_dwconv 得到 Q、K、V，并进行拆分。
#     #     qkv = self.qkv_dwconv(self.qkv(x))
#     #     q1, k1, q2, k2, v = qkv.chunk(5, dim=1)  # b,c,x,x
#     #
#     #     # 3.排序后的聚合：根据排序索引 idx 对 Q、K、V 进行重排，确保它们在空间维度上符合要求。
#     #     v, idx = v.view(b, c, -1).sort(dim=-1)
#     #     q1 = torch.gather(q1.view(b, c, -1), dim=2, index=idx)
#     #     k1 = torch.gather(k1.view(b, c, -1), dim=2, index=idx)
#     #     q2 = torch.gather(q2.view(b, c, -1), dim=2, index=idx)
#     #     k2 = torch.gather(k2.view(b, c, -1), dim=2, index=idx)
#     #
#     #     # 4.注意力计算：使用 reshape_attn 方法分别计算局部（out1）和全局（out2）注意力。
#     #     out1 = self.reshape_attn(q1, k1, v, True)
#     #     out2 = self.reshape_attn(q2, k2, v, False)
#     #
#     #     # 5.结果合并与恢复：将两个注意力结果 out1 和 out2 相乘，并进行投影输出。最后，使用 scatter 和索引操作将结果恢复到原始顺序。
#     #     out1 = torch.scatter(out1, 2, idx, out1).view(b, c, h, w)
#     #     out2 = torch.scatter(out2, 2, idx, out2).view(b, c, h, w)
#     #     out = out1 * out2
#     #     out = self.project_out(out)
#     #     out_replace = out[:, :c // 2]
#     #     out_replace = torch.scatter(out_replace, -1, idx_w, out_replace)
#     #     out_replace = torch.scatter(out_replace, -2, idx_h, out_replace)
#     #     out[:, :c // 2] = out_replace
#     #     return out
#
#
# class TransformerEncoderLayer_DHSA(nn.Module):
#     """Defines a single layer of the transformer encoder."""
#
#     def __init__(self, c1, cm=2048, num_heads=8, dropout=0.0, act=nn.GELU(), normalize_before=False):
#         """Initialize the TransformerEncoderLayer with specified parameters."""
#         super().__init__()
#         self.additivetoken = Attention_histogram(c1, num_heads)
#         # Implementation of Feedforward model
#         self.fc1 = nn.Conv2d(c1, cm, 1)
#         self.fc2 = nn.Conv2d(cm, c1, 1)
#
#         self.norm1 = LayerNorm(c1)
#         self.norm2 = LayerNorm(c1)
#         self.dropout = nn.Dropout(dropout)
#         self.dropout1 = nn.Dropout(dropout)
#         self.dropout2 = nn.Dropout(dropout)
#
#         self.act = act
#         self.normalize_before = normalize_before
#
#     def forward_post(self, src, src_mask=None, src_key_padding_mask=None, pos=None):
#         """Performs forward pass with post-normalization."""
#         src2 = self.additivetoken(src)
#         src = src + self.dropout1(src2)
#         src = self.norm1(src)
#         src2 = self.fc2(self.dropout(self.act(self.fc1(src))))
#         src = src + self.dropout2(src2)
#         return self.norm2(src)
#
#     def forward(self, src, src_mask=None, src_key_padding_mask=None, pos=None):
#         """Forward propagates the input through the encoder module."""
#         return self.forward_post(src, src_mask, src_key_padding_mask, pos)

import torch
import torch.nn as nn
import torch.nn.functional as F

from einops import rearrange

class LayerNorm(nn.Module):
    """ LayerNorm that supports two data formats: channels_last (default) or channels_first.
    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with
    shape (batch_size, height, width, channels) while channels_first corresponds to inputs
    with shape (batch_size, channels, height, width).
    """

    def __init__(self, normalized_shape, eps=1e-6, data_format="channels_first"):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = eps
        self.data_format = data_format
        if self.data_format not in ["channels_last", "channels_first"]:
            raise NotImplementedError
        self.normalized_shape = (normalized_shape,)

    def forward(self, x):
        if self.data_format == "channels_last":
            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        elif self.data_format == "channels_first":
            u = x.mean(1, keepdim=True)
            s = (x - u).pow(2).mean(1, keepdim=True)
            x = (x - u) / torch.sqrt(s + self.eps)
            x = self.weight[:, None, None] * x + self.bias[:, None, None]
            return x



class Attention_histogram(nn.Module):
    def __init__(self, dim, num_heads=8, bias=False, ifBox=True):
        super(Attention_histogram, self).__init__()
        self.factor = num_heads
        self.ifBox = ifBox
        self.num_heads = num_heads
        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))

        self.qkv = nn.Conv2d(dim, dim * 5, kernel_size=1, bias=bias)
        self.qkv_dwconv = nn.Conv2d(dim * 5, dim * 5, kernel_size=3, stride=1, padding=1, groups=dim * 5, bias=bias)
        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)

    def pad(self, x, factor):
        hw = x.shape[-1]
        t_pad = [0, 0] if hw % factor == 0 else [0, (hw // factor + 1) * factor - hw]
        x = F.pad(x, t_pad, 'constant', 0)
        return x, t_pad

    def unpad(self, x, t_pad):
        _, _, hw = x.shape
        return x[:, :, t_pad[0]:hw - t_pad[1]]

    def softmax_1(self, x, dim=-1):
        logit = x.exp()
        logit = logit / (logit.sum(dim, keepdim=True) + 1)
        return logit

    def normalize(self, x):
        mu = x.mean(-2, keepdim=True)
        sigma = x.var(-2, keepdim=True, unbiased=False)
        return (x - mu) / torch.sqrt(sigma + 1e-5)  # * self.weight + self.bias

    def reshape_attn(self, q, k, v, ifBox):
        b, c = q.shape[:2]
        q, t_pad = self.pad(q, self.factor)
        k, t_pad = self.pad(k, self.factor)
        v, t_pad = self.pad(v, self.factor)
        hw = q.shape[-1] // self.factor
        shape_ori = "b (head c) (factor hw)" if ifBox else "b (head c) (hw factor)"
        # 当ifBox = True时，通过将空间维度分成块，可以更好的捕获局部信息，有助于提高模型对局部特征的关注，从而可能减少计算复杂度
        # 当ifBox = False时，数据保持了全局的空间结构，这样可以更好的捕捉长距离依赖关系，可能会增加计算量，适合需要考虑全局上下文的情况
        shape_tar = "b head (c factor) hw"
        q = rearrange(q, '{} -> {}'.format(shape_ori, shape_tar), factor=self.factor, hw=hw, head=self.num_heads)
        k = rearrange(k, '{} -> {}'.format(shape_ori, shape_tar), factor=self.factor, hw=hw, head=self.num_heads)
        v = rearrange(v, '{} -> {}'.format(shape_ori, shape_tar), factor=self.factor, hw=hw, head=self.num_heads)
        q = torch.nn.functional.normalize(q, dim=-1)
        k = torch.nn.functional.normalize(k, dim=-1)
        attn = (q @ k.transpose(-2, -1)) * self.temperature
        attn = self.softmax_1(attn, dim=-1)
        out = (attn @ v)
        out = rearrange(out, '{} -> {}'.format(shape_tar, shape_ori), factor=self.factor, hw=hw, b=b,
                        head=self.num_heads)
        out = self.unpad(out, t_pad)
        return out

    def forward(self, x):
        b, c, h, w = x.shape
        x_sort, idx_h = x[:, :c // 2].sort(-2)
        x_sort, idx_w = x_sort.sort(-1)
        x[:, :c // 2] = x_sort
        qkv = self.qkv_dwconv(self.qkv(x))
        q1, k1, q2, k2, v = qkv.chunk(5, dim=1)  # b,c,x,x

        v, idx = v.view(b, c, -1).sort(dim=-1)
        q1 = torch.gather(q1.view(b, c, -1), dim=2, index=idx)
        k1 = torch.gather(k1.view(b, c, -1), dim=2, index=idx)
        q2 = torch.gather(q2.view(b, c, -1), dim=2, index=idx)
        k2 = torch.gather(k2.view(b, c, -1), dim=2, index=idx)

        out1 = self.reshape_attn(q1, k1, v, True)
        out2 = self.reshape_attn(q2, k2, v, False)

        out1 = torch.scatter(out1, 2, idx, out1).view(b, c, h, w)
        out2 = torch.scatter(out2, 2, idx, out2).view(b, c, h, w)
        out = out1 * out2
        out = self.project_out(out)
        out_replace = out[:, :c // 2]
        out_replace = torch.scatter(out_replace, -1, idx_w, out_replace)
        out_replace = torch.scatter(out_replace, -2, idx_h, out_replace)
        out[:, :c // 2] = out_replace
        return out


class TransformerEncoderLayer_DHSA(nn.Module):
    """Defines a single layer of the transformer encoder."""

    def __init__(self, c1, cm=2048, num_heads=8, dropout=0.0, act=nn.GELU(), normalize_before=False):
        """Initialize the TransformerEncoderLayer with specified parameters."""
        super().__init__()
        self.additivetoken = Attention_histogram(c1, num_heads)
        # Implementation of Feedforward model
        self.fc1 = nn.Conv2d(c1, cm, 1)
        self.fc2 = nn.Conv2d(cm, c1, 1)

        self.norm1 = LayerNorm(c1)
        self.norm2 = LayerNorm(c1)
        self.dropout = nn.Dropout(dropout)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

        self.act = act
        self.normalize_before = normalize_before

    def forward_post(self, src, src_mask=None, src_key_padding_mask=None, pos=None):
        """Performs forward pass with post-normalization."""
        src2 = self.additivetoken(src)
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.fc2(self.dropout(self.act(self.fc1(src))))
        src = src + self.dropout2(src2)
        return self.norm2(src)

    def forward(self, src, src_mask=None, src_key_padding_mask=None, pos=None):
        """Forward propagates the input through the encoder module."""
        return self.forward_post(src, src_mask, src_key_padding_mask, pos)
